model_backend: llama_cpp # options: gpt4all | llama_cpp | mistral
model_path: ./models/llm/mistral-7b-instruct-v0.2.Q4_K_M.gguf
top_k: 5
max_tokens: 512
temperature: 0.2
